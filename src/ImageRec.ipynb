{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImageRec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grzabrodskiy/ImageRec/blob/main/src/ImageRec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rVDkKZEEKIp"
      },
      "source": [
        "Preparing to use **Google Colab**\n",
        "\n",
        "Based on the course labs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Data Storage"
      ],
      "metadata": {
        "id": "4lr9HMqlCJ7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ps -elf | grep python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfhNkU0ggFzM",
        "outputId": "eec24669-48f5-448c-9f97-6425eee03f92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 Z root          35       8  2  80   0 -     0 -      21:46 ?        00:00:04 [python3] <defunct>\n",
            "4 S root          36       8  0  80   0 - 39435 epoll_ 21:46 ?        00:00:00 python3 /usr/local/bin/colab-fileshim.py\n",
            "4 S root          61       8  0  80   0 - 50354 epoll_ 21:46 ?        00:00:01 /usr/bin/python3 /usr/local/bin/jupyter-notebook --ip=\"172.28.0.2\" --port=9000 --FileContentsManager.root_dir=\"/\" --MappingKernelManager.root_dir=\"/content\"\n",
            "4 S root          79      61  4  80   0 - 118423 select 21:49 ?       00:00:01 /usr/bin/python3 -m ipykernel_launcher -f /root/.local/share/jupyter/runtime/kernel-6dc108e3-3dd2-40d1-aeb8-2c3e2f5c4080.json\n",
            "1 S root          99       1  0  80   0 - 31476 futex_ 21:49 ?        00:00:00 /usr/bin/python3 /usr/local/lib/python3.7/dist-packages/debugpy/adapter --for-server 35897 --host 127.0.0.1 --port 19422 --server-access-token a9312c37bf442c787d4f723fcb630ed79003c5b21d5eacae40ba0997184df7b6\n",
            "0 S root         136      79  0  80   0 -  9799 wait   21:49 ?        00:00:00 /bin/bash -c ps -elf | grep python\n",
            "0 S root         138     136  0  80   0 -  9643 pipe_r 21:49 ?        00:00:00 grep python\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!kill -9 1938"
      ],
      "metadata": {
        "id": "tC-Wo0uYgP1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISE0pBnS6nC6"
      },
      "source": [
        "1. Setting up the device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-vsUDzAJwUU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1036a4d7-1633-4d2a-f5bd-e9f4f4cce66c"
      },
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJy9xDAo4lFv"
      },
      "source": [
        "2. Mount Gooogle Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kl3ZyAAVEKI1",
        "outputId": "6aa22ce4-a455-43e4-d412-06f77e326d5e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxIKS3ukEKI2"
      },
      "source": [
        "3. Setting up the directories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28VgE7dMEKI2"
      },
      "source": [
        "import os\n",
        "\n",
        "GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', 'School')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Unzipoing the dataset"
      ],
      "metadata": {
        "id": "y8uW7rLrB11z"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViNfh2MiM0Bw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2c2b1c1-0f97-4dc5-ea2b-05ccc41dc9be"
      },
      "source": [
        "# Identify path to zipped dataset\n",
        "zip_path = os.path.join(GOOGLE_DRIVE_PATH, 'CW_Dataset.zip')\n",
        "# Copy it to Colab\n",
        "!cp '{zip_path}' .\n",
        "\n",
        "# Unzip it (removing useless files stored in the zip)\n",
        "!unzip -q CW_Dataset.zip -d -o CW_Dataset\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "caution: filename not matched:  CW_Dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up datasets"
      ],
      "metadata": {
        "id": "JM213-yyeoAP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEO5XznBMBkV"
      },
      "source": [
        "# 1: Transfer Learning\n",
        "\n",
        "In this first part, we will learn how to train a convolutional neural network for\n",
        "image classification using **transfer learning**. We have briefly encountered this concept in a previous lecture, but you can read more about it on these [note pages](https://cs231n.github.io/transfer-learning/).\n",
        "\n",
        "Quoting these notes,\n",
        "\n",
        "> In practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest.\n",
        "\n",
        "These two major transfer learning scenarios look as follows:\n",
        "-  **Finetuning the CNN**: instead of random initialization, we\n",
        "   initialize the network with a pretrained network, like the one that is trained on ImageNet. Since we will very likely have a different number of categories compared to the 1000 of ImageNet, we will usually need to replace the final fully-connected layer with a new one (with random weights) having the correct number of neurons. Then the rest of the training looks as usual.\n",
        "-  **CNN as fixed feature extractor**: here, we will freeze the weights for all of the network except that of the final fully-connected layer, which will be replaced with a new one (with random weights and the correct number of neurons) as in the previous case. Only this layer is trained. The coding for this second approach will constitute a Task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9WuXUHGbfCB"
      },
      "source": [
        "# License: BSD\n",
        "# Author: Sasank Chilamkurthy\n",
        "# Modified by: Giacomo Tarroni\n",
        "# Modified by: Greg Zabrodskiy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import copy\n",
        "import shutil\n",
        "from torchvision.utils import make_grid\n",
        "import random\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QNr7As4bfCC"
      },
      "source": [
        "## Loading the Data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvmWknrc0vdV"
      },
      "source": [
        "#data_dir = 'CW_Dataset'\n",
        "#data_means = [0.485, 0.456, 0.406]\n",
        "#data_stds = [0.229, 0.224, 0.225]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nyt-c1h0wp-"
      },
      "source": [
        "The `data_means` and `data_stds` are lists with the means and standard deviations of the pixel intensities of the three colour channels in the dataset, respectively. They will be useful to normalize the images to the [-1, 1] range that is best used with training CNNs.\n",
        "\n",
        "Data transformation is going to be different between training and validation phases:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlZIqC6rbfCC"
      },
      "source": [
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation and testing\n",
        "data_transforms = {\n",
        "\n",
        "  \n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.Grayscale(num_output_channels=3),\n",
        "\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(0.5, 0.5)\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(0.5, 0.5, inplace=True)\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(0.5, 0.5, inplace=True)\n",
        "    ]),\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up label names"
      ],
      "metadata": {
        "id": "PO14Mpn9Cw7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['', 'Surprise', 'Fear', 'Disgust', 'Happiness', 'Sadness', 'Anger', 'Neutral']"
      ],
      "metadata": {
        "id": "WJIul9dE2hSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create directory structure for pytorch standard loader"
      ],
      "metadata": {
        "id": "rj_mOvdWC2cT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files = {}\n",
        "for t in ['test', 'train', 'val']:\n",
        "  for n in range (1, 8):\n",
        "    dirpath = f'CW_Dataset/{t}/sorted/' + class_names[n]\n",
        "    if os.path.exists(dirpath) and os.path.isdir(dirpath):\n",
        "      shutil.rmtree(dirpath)\n",
        "    os.makedirs(dirpath, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "Nu8bko921RXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "val_data_size = 0.2 # use this percent value of train data for validation\n",
        "\n",
        "for t in ['test', 'train']:\n",
        "  files[t] = np.genfromtxt(f'CW_Dataset/labels/list_label_{t}.txt',dtype='str')\n",
        "\n",
        "  for fl in files[t]:\n",
        "    if fl[0].endswith('.jpg'):\n",
        "      if (t == 'train') and (random.uniform(0, 1) < val_data_size):\n",
        "        shutil.copy2(f'CW_Dataset/{t}/' + fl[0][:-4] + '_aligned.jpg', f'CW_Dataset/val/sorted/' + class_names[int(fl[1])] + '/' + fl[0][:-4] +  '_aligned.jpg')\n",
        "      else:\n",
        "        shutil.copy2(f'CW_Dataset/{t}/' + fl[0][:-4] + '_aligned.jpg', f'CW_Dataset/{t}/sorted/' + class_names[int(fl[1])] + '/' + fl[0][:-4] +  '_aligned.jpg')\n"
      ],
      "metadata": {
        "id": "ORDgrmlQvtO2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "outputId": "a88ffef0-a5db-48d5-daa5-ab36e6a3e4a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-30c906c66ee4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'CW_Dataset/labels/list_label_{t}.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'str'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mfl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, like)\u001b[0m\n\u001b[1;32m   1791\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1793\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1794\u001b[0m             \u001b[0mfid_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1795\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    531\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    532\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: CW_Dataset/labels/list_label_test.txt not found."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lf3oEowBMKWc"
      },
      "source": [
        "Set up datasets and dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkUpF9xtMRfv"
      },
      "source": [
        "image_dataset = {}\n",
        "dataloader = {}\n",
        "for t in ['test', 'train', 'val']:\n",
        "\n",
        "  image_dataset[t] = datasets.ImageFolder(f'/content/CW_Dataset/{t}/sorted', data_transforms[t])\n",
        "\n",
        "  dataloader[t] = torch.utils.data.DataLoader(image_dataset[t], batch_size=4,\n",
        "                                             shuffle=True, num_workers=2)\n",
        "            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4Wwspe8LRS6"
      },
      "source": [
        "Note that here both `image_datasets` and `dataloaders` have been created as dictionaries, with two elements representing the training and validation datasets, respectively.\n",
        "\n",
        "We can also define the following variables, which will be of use for the rest of the code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lheMzW24AWiV"
      },
      "source": [
        "dataset_sizes = {x: len(image_dataset[x]) for x in ['train', 'test', 'val']}\n",
        "print(dataset_sizes)\n",
        "class_names = image_dataset['train'].classes\n",
        "print(class_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-p86mISbfCD"
      },
      "source": [
        "## Visualizing some images\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xjx_yyX9bfCD"
      },
      "source": [
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    inp = np.asarray(0.5) * inp + np.asarray(0.5)\n",
        "    inp = np.clip(inp, 0, 1)       # Clip to intensity outliers to [0, 1] range\n",
        "    plt.imshow(inp)\n",
        "    plt.title(title)\n",
        "\n",
        "\n",
        "# Get a batch of training data\n",
        "for i in range (1, 5):\n",
        "  inputs, classes = next(iter(dataloader['train']))\n",
        "\n",
        "  # Make a grid from batch\n",
        "  out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "  plt.figure(figsize=(5, 10))\n",
        "  imshow(out, title=[class_names[x] for x in classes])\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z73UHkcfbfCD"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "Now, let's write a general function to train a model. The function will:\n",
        "\n",
        "- Run for a given number of epochs\n",
        "- For each epoch, there will be a training phase (where the model will be updated from the previous epoch) and a validation phase (where it will be applied to the validation set for prediction)\n",
        "- Save the best model (based on the performance on the validation set)\n",
        "\n",
        "Importantly, we will also **schedule the learning rate** (LR) (i.e. change it during training). In the function, the parameter ``scheduler`` is an LR scheduler object from [``torch.optim.lr_scheduler``](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNx6bsBqbfCE"
      },
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch + 1} out of {num_epochs}\")\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data\n",
        "            for inputs, labels in dataloader[phase]:\n",
        "                # move data to GPU\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # update learning rate with scheduler\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print(f\"{phase} loss: {epoch_loss:.4f} acc: {epoch_acc:.4f}\")\n",
        "\n",
        "            # deep copy the model with best accuracy on validation set\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f\"Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n",
        "    print(f\"Best val acc: {best_acc:4f}\")\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV5uZ2Q3bfCE"
      },
      "source": [
        "## Visualizing the model predictions\n",
        "\n",
        "Let's also define a generic function to use the model to predict the classes of a few images (from the validation set) and to visualize the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUXwo_wLbfCF"
      },
      "source": [
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloader['val']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 7, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title(f\"Predicted: {class_names[preds[j]]}\")\n",
        "                imshow(inputs.cpu().data[j])        # move back the data to the CPU for visualisation\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xhOx3r2bfCG"
      },
      "source": [
        "## Finetuning the CNN\n",
        "\n",
        "Finetuning a CNN consists in taking a model (i.e. **architecture and weights**) pre-trained on a given dataset (very often ImageNet) and using it as a starting point for training on a different dataset. The steps involved in finetuning are the following:\n",
        "\n",
        "1. Load a pre-trained model\n",
        "2. Reset the final layer (technically needed only if the number of classes is different between the two datasets, but generally always a good idea)\n",
        "3. Perform training on the new dataset\n",
        "\n",
        "In this example, we will first download the *ResNet18* model from the model zoo of `torchvision` (more information on how this works can be found [here](https://pytorch.org/vision/stable/models.html)). Then, we will reset the last fully-connected layer and perform training on our own dataset.\n",
        "\n",
        "Download the pre-trained model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAavKIqrXp87"
      },
      "source": [
        "model_ft = models.resnet18(pretrained=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4TE-1pWXr50"
      },
      "source": [
        "Create a new fully-connected layer and use it to replace the original one of *Resnet18*. It will need to have the same input dimensions as the original one, but have an output size equal to the number of classes in our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCxTIXQgbfCG"
      },
      "source": [
        "num_ftrs = model_ft.fc.in_features\n",
        "model_ft.fc = nn.Linear(num_ftrs, 7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnCSlWHbYC8W"
      },
      "source": [
        "Then we can move the model to the GPU and set the remaining training parameters and criterion (i.e. the loss function):"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "id": "UOIKRauadzYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svaPxyjObfCG"
      },
      "source": [
        "## Train and evaluate\n",
        "\n",
        "Now we can actually train our model. It should take around 15-25 min on CPU. On GPU though, it takes less than a minute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNOS5Q1XbfCH"
      },
      "source": [
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBed0LclYdCt"
      },
      "source": [
        "Let's test our final result on some images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY3plJKmbfCI"
      },
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "visualize_model(model_ft)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNQ1QIzmGb0b"
      },
      "source": [
        "torch.save(model_ft.state_dict(), 'CNN.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akYzvJSHGglF"
      },
      "source": [
        "You should now see the model file in the *Files* folder in the toolbar on the left side. \n",
        "\n",
        "PyTorch offers other ways to achieve model persistence and also provides an easy way to **load directly on CPU models that were trained on GPU** (allowing testing on machines without acceleration). To learn more about these topics, make sure to visit this [tutorial](https://pytorch.org/tutorials/beginner/saving_loading_models.html)."
      ]
    }
  ]
}